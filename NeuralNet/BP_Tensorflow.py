# encoding:utf-8
'''
    ğŸ•¸ ç´¯ç§¯BPä¸æ ‡å‡†BP
'''
import tensorflow as tf
import sys
import time
from os import path
sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))
from dataSet.watermelon_3 import wm_trainningset, wm_validationset
from itertools import compress

LEARNING_RATE = 0.05
HIDDEN_LAYER_ORDER = 8

'''
ä¿®æ”¹æ•°æ®æ ¼å¼ä¸º:
(((x11, x12, x13), (x21, x22, x23)...), ((y1), (y2), (y3)...))
'''

wm_trainningset = map(lambda item:tuple((item[0], (item[1],))) ,wm_trainningset)
wm_trainningset = tuple(zip(*wm_trainningset))
wm_validationset = map(lambda item:tuple((item[0], (item[1],))) ,wm_validationset)
wm_validationset = tuple(zip(*wm_validationset))

def addlayer(prelayer, inputshape, outputshape, activate_func=None, name="layer"):
    '''
        æ·»åŠ ä¸­é—´å±‚
    '''
    weighs = tf.Variable(tf.random_normal([inputshape, outputshape]), name=name + '_w')
    bias = tf.Variable(tf.zeros([1, outputshape]), name=name + '_b')
    x_mul_w_plus_b = tf.matmul(prelayer, weighs) + bias
    if activate_func is None:   #Pythonä¸­çš„å¯¹è±¡åŒ…å«ä¸‰è¦ç´ ï¼šidã€typeã€valueã€‚isåˆ¤æ–­é€šè¿‡idæ¥åˆ¤æ–­çš„ã€‚==é€šè¿‡valueæ¥åˆ¤æ–­çš„ã€‚
        return x_mul_w_plus_b
    return activate_func(x_mul_w_plus_b)


def main():
    '''
        ä¸»å‡½æ•°
    '''
    # 1 å®šä¹‰å˜é‡
    feature_space = tf.placeholder("float", [None, 8])
    label_space = tf.placeholder("float", [None, 1])
    # 2 å®šä¹‰å±‚
    hiddenlayer = addlayer(feature_space, 8, HIDDEN_LAYER_ORDER, tf.nn.sigmoid, "hiddenlayer")
    prediction = addlayer(hiddenlayer, HIDDEN_LAYER_ORDER, 1, tf.nn.sigmoid, "outputlayer")
    # 3 å®šä¹‰è¯„ä»·å‡½æ•°ä¸optimizer
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(prediction - label_space), axis=[1]))
    # 4 è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®ç‡
    error_rate = tf.reduce_mean(tf.abs(tf.cast(tf.greater(prediction, 0.5), "float")-label_space))
    with tf.name_scope('summaries1'):
        tf.summary.scalar("loss:", loss)
    train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        step, minval = 0, 1
        while True:
            sess.run(train_step, feed_dict={feature_space:wm_trainningset[0], label_space:wm_trainningset[1]})
            cur = sess.run(loss, feed_dict={feature_space:wm_validationset[0], label_space:wm_validationset[1]})
            if step%100 == 0:
                print("ç¬¬%dæ¬¡æµ‹è¯•é›†è¯¯å·®:%.6f" % (step, cur))
            if cur < minval:
                minval = cur
                accuracy = 1 - sess.run(error_rate, feed_dict={feature_space:wm_validationset[0], label_space:wm_validationset[1]})
                count = step
            elif (cur - minval) > 0.1:
                print("ç´¯ç§¯è®­ç»ƒæœ€ç»ˆè¯¯å·®%f" % minval)
                break
            step = step+1

        #æ‰“ä¸€ä¸ªè®­ç»ƒæ—¥å¿—
        with open("./BPNN.log", mode="a") as logfile:
            logfile.writelines(time.strftime('%m-%d %Hu:%M', time.localtime(time.time()))+"\n")
            logfile.writelines(u"  å­¦ä¹ ç‡ä¸º{learningrate}, éšå«å±‚ç»´åº¦ä¸º{order}, æœ€ç»ˆæµ‹è¯•é›†è¯¯å·®ä¸º{loss:.5f}, è®­ç»ƒäº†{count}æ¬¡, æ­£ç¡®ç‡ä¸º:{accu:.5f}\n"\
            .format(learningrate=LEARNING_RATE, order=HIDDEN_LAYER_ORDER, loss=minval, count=count, accu=accuracy))

if __name__ == '__main__':
    main()
