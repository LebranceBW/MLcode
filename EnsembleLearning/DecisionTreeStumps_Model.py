#encoding:utf-8
'''
    æ¥å—å¸¦æƒå€¼çš„æ•°æ®é›†å¹¶å®ç°ä¸€ä¸ªå†³ç­–æ ‘æ¡©ğŸŒ²
'''
from functools import partial
from enum import Enum
class Decide(Enum):
    '''
    ç¬”è®°ï¼šdecide.greater_to_positive ä¹Ÿæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼ä¸èƒ½ç›´æ¥ç”¨æ¥å’Œå€¼è¿›è¡Œæ¯”è¾ƒ
        Decide.greater_to_positive.value è¿”å›å…¶å®é™…å€¼
    '''
    greater_to_positive = True
    lesser_to_positive = False

def generate_stumps(dataset, weights=None):
    '''
        ç”Ÿæˆå†³ç­–æ ‘æ¡©ï¼Œè¾“å…¥datasetæ ¼å¼ä¸ºï¼š
        [
            [[x11,x12],y1],
            [[x21,x22],y2]
            ...
        ]
        weights ä¸ºè¾“å…¥çš„æƒå€¼
        [w1, w2, w3...]
        è¿”å›ï¼š
        ï¼ˆåˆ†ç±»å™¨ï¼Œï¼ˆé”™è¯¯ç‡ï¼Œåˆ†ç•Œç‚¹ï¼Œåˆ†ç•Œç­–ç•¥,å±æ€§ä¸‹æ ‡ï¼‰ï¼‰
    '''
    if weights == None:
        weights = [1/len(dataset)] * len(dataset)

    def each_attri_best(i, dataset):
        '''
            è¿”å›æŸä¸€å±æ€§ä¸­æœ€ä½³çš„é”™è¯¯ç‡å’Œåˆ†ç±»ç­–ç•¥ï¼Œiä¸ºå±æ€§ä¸‹æ ‡ï¼Œdatasetä¸ºzipped_set
            è¿”å›æ ¼å¼ï¼š(é”™è¯¯ç‡ï¼Œåˆ†ç•Œç‚¹ï¼Œåˆ†ç•Œç­–ç•¥,å±æ€§ä¸‹æ ‡)
        '''
        itemset = ([item[0][i], item[1], item[2]] for item in dataset)
        '''
            æ ¼å¼[xi ,y, weight]
        '''
        sorted_dataset = sorted(itemset, key=lambda item: item[0])
        divide_points = map(lambda x, y: float(format((x[0]+y[0])/2, ".3f")), sorted_dataset, sorted_dataset[1:])#æ±‚ç›¸é‚»ä¸¤å±æ€§å€¼çš„ä¸­é—´å€¼,ç§»ä½
        error_rates = map(partial(each_dividepoint, dataset=sorted_dataset), divide_points)
        local_best = sorted(error_rates, key=lambda item: item[0])[0]
        return (*local_best, i)

    def each_dividepoint(point, dataset):
        '''
            dataset ä¸ºæ•°æ®é›†ï¼ˆsorted_dataset)ï¼Œpointä¸ºä¸€ä¸ªç‚¹
            è¿”å›å„ä¸ªåˆ†ç±»ç‚¹çš„å¸¦æƒè¯¯å·®
            é»˜è®¤è®¤ä¸ºå¤§äºä¸´ç•Œä¸ºæ­£ä¾‹ï¼ˆgreater_to_positive)
            è¿”å›å€¼ä¸ºï¼ˆé”™è¯¯ç‡ï¼Œåˆ†ç•Œç‚¹ï¼Œåˆ†ç•Œç­–ç•¥ï¼‰
        '''
        # def xor(a, b):
        #     #å¼‚æˆ–é€»è¾‘
        #     if bool(a) != bool(b): return True
        #     return False
        error_rate = sum(map(lambda item: item[2]*(1 if (item[0] < point) != (item[1] == -1) else 0), dataset))
        '''
            å¦‚æœè¯¥å…ƒç´ åœ¨ä¸´ç•Œç‚¹å·¦ä¾§ä¸”ä¸ºæ­£ä¾‹åˆ™ç»Ÿè®¡å…¶æƒå€¼ï¼Œåœ¨ä¸´ç•Œç‚¹å³ä¾§ä¸”ä¸ºåä¾‹çš„åŒç†
        '''
        if error_rate > 0.5:#å¦‚æœé”™è¯¯ç‡å¤§äº0.5ï¼Œé‚£ä¹ˆè¯´æ˜åˆ†ç±»ç­–ç•¥åäº†
            return (1-error_rate, point, Decide.lesser_to_positive)
        return (error_rate, point, Decide.greater_to_positive)
    zipped_dataset = list(map(lambda data, weight: (*data, weight), dataset, weights))
    # [[*data, weight] for data in dataset for weight in weights]
    '''
      æŠŠæ•°æ®é›†å’Œæƒå€¼æ‰“åŒ… zipped_dataset æ ¼å¼  [[x11,x12],y1,w1],...

    '''
    attri_numbers = len(dataset[0][0])
    best_classifier_pack = sorted(map(partial(each_attri_best, dataset=zipped_dataset), range(attri_numbers)), key=lambda item: item[0])[0]
    '''
        packæ ¼å¼ï¼š(é”™è¯¯ç‡ï¼Œåˆ†ç•Œç‚¹ï¼Œåˆ†ç•Œç­–ç•¥,å±æ€§ä¸‹æ ‡)
    '''

    def weak_classifier(feature_vector):
        '''
            ç”¨äºè¿”å›çš„å¼±åˆ†ç±»å‡½æ•°
            è¾“å…¥ç‰¹å¾å‘é‡ï¼Œè¾“å‡ºæ ‡ç­¾
        '''
        if (feature_vector[best_classifier_pack[-1]] > best_classifier_pack[1]) == (best_classifier_pack[2].value):
            return 1
        return -1

    return weak_classifier, best_classifier_pack

def test_stumps(dataset):
    '''
        å•å…ƒæµ‹è¯•æ¨¡å—ï¼ŒTODO
    '''
    print(generate_stumps(dataset, [1/17]*17))

if __name__ == '__main__':
    print("è¿™åªæ˜¯ä¸ªæ¨¡å—")
